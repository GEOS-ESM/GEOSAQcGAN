# Copyright 2025, The Johns Hopkins University Applied Physics Laboratory LLC.
# All rights reserved.
# Distributed under the terms of the BSD 3-Clause License.

"""
This script will generate "predictions" for a test dataset at a given vertical level.
The "predictions" will be both the mean and the spread across all ensemble members.
These "predictions" can be an arbitrary number of timesteps in the future compared to the input
meaning that we can generate autoregressive predictions (e.g. 10 days out)
The "predictions" can be one of three things:
    1. Ground truth
    2. Persistence curve predictions
    3. Predictions generated by a trained AQcGAN model
Later on all 3 versions of these predictions can be loaded by a different script which can
use the ground truth and the model (or persistence) predictions to calculate error and evaluate
performance of the AQcGAN model (and potentially compare it to persistence curves as a baseline)

This script has to be run from the root directory of the repository with the command
`python -m inference.create_ensemble_predictions`
"""

import os
import sys
import argparse
from pathlib import Path
import pickle

import numpy as np
import torch

from ..shared.gen_utils import read_pickle_file
from ..shared.aqcgan_utils import parse_yaml_prediction
from .utils import get_norm_stats
from ..shared.aqcgan_utils import get_predictions
from ..shared.aqcgan_utils import inv_pred

DEVICE = 0 if torch.cuda.is_available() else "cpu"

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config_filepath", type=str, help="Full path to the experiment config file")
    parser.add_argument("chkpt_idx", type=int, help="Model checkpoint.")
    parser.add_argument("meta_filepath", type=str, help="Full path to the Pickle file containing experiment data.")
    parser.add_argument("--n_passes", "-n", type=int, default=1, help="Number of forward passes to run.")
    parser.add_argument("--vertical_level", type=int, default=72, help="Vertical level to evaluate on")
    args = parser.parse_args()

    # load trained AQcGAN model
    config_filepath = Path(args.config_filepath)
    trainer = parse_yaml_prediction(config_filepath, args.chkpt_idx, device=DEVICE)
    trainer.gen.eval()

    # get meta data
    meta_data = read_pickle_file(args.meta_filepath)
    if not meta_data:
        sys.exit()

    # get dataset
    dataset = trainer.val_data

    old_feats = dataset.feat_names
    old_targets = dataset.target_names
    new_feats = {}
    new_targets = {}

    # Ensuring dataset has single vertical level (required for evaluation)
    # Remapping the correct input/target features to be for this vertical level 
    level = list(old_feats.keys())[0]
    new_feats = {args.vertical_level: []}
    for x in old_feats[level]:
        new_x = x.replace(str(level), str(args.vertical_level))
        # features that are vertical level dependent have the name "variable_level"
        # get the features for the new vertical level
        if new_x in meta_data['z_vars']:
            new_feats[args.vertical_level].append(new_x)
        else:
            new_feats[args.vertical_level].append(x)

    # repeat for target/output variables
    new_targets = {args.vertical_level: []}
    for x in old_targets[level]:
        new_x = x.replace(str(level), str(args.vertical_level))
        if new_x in meta_data['z_vars']:
            new_targets[args.vertical_level].append(new_x)
        else:
            new_targets[args.vertical_level].append(x)

    # use these to evaluate model on the desired vertical level
    # (potentially different than what it was trained on)
    dataset.dataset.setup_feats_targets(new_feats, new_targets)

    # Get normalization stats
    mu, sigma = get_norm_stats(dataset.target_names[args.vertical_level], meta_data)

    save_file = f"fc_pred_stats_days{args.n_passes}_level{args.vertical_level}.npz"

    if os.path.exists(trainer.chkpt_dir / save_file):
        print(f'{save_file} already exists! Skipping')
        exit()

    # returns mean and variance in normalized space across ensembles for each example in test set
    predictions = get_predictions(trainer, n_passes=args.n_passes)

    # invert mean and spread to get mean and std deviation in native space
    predictions_inv = inv_pred(predictions.numpy(), mu, sigma)

    # save "predictions"
    save_dict = {
        f"pred_inv": predictions_inv,
    }
    with open(trainer.chkpt_dir / save_file, "wb") as fid:
        np.savez(fid, **save_dict)

