# Copyright 2025, The Johns Hopkins University Applied Physics Laboratory LLC.
# All rights reserved.
# Distributed under the terms of the BSD 3-Clause License.

"""
This script will generate "predictions" for a test dataset at a given vertical level.
The "predictions" will be both the mean and the spread across all ensemble members.
These "predictions" can be an arbitrary number of timesteps in the future compared to the input
meaning that we can generate autoregressive predictions (e.g. 10 days out)
The "predictions" can be one of three things:
    1. Ground truth
    2. Persistence curve predictions
    3. Predictions generated by a trained AQcGAN model
Later on all 3 versions of these predictions can be loaded by a different script which can
use the ground truth and the model (or persistence) predictions to calculate error and evaluate
performance of the AQcGAN model (and potentially compare it to persistence curves as a baseline)

This script has to be run from the root directory of the repository with the command
`python -m inference.create_ensemble_predictions`
"""

import os
import sys
import argparse
from pathlib import Path
import pandas as pd
import xarray as xr
import numpy as np
import torch

from ..shared.gen_utils import read_pickle_file
from .utils import get_norm_stats
from ..shared.aqcgan_utils import get_predictions
from ..shared.aqcgan_utils import inv_pred
from ..train_ens_ic import parse_yaml

DEVICE = 0 if torch.cuda.is_available() else "cpu"

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config_filepath", type=str, help="Full path to the experiment config file")
    parser.add_argument("chkpt_idx", type=int, help="Model checkpoint.")
    parser.add_argument("meta_filepath", type=str, help="Full path to the Pickle file containing experiment data.")
    parser.add_argument("--n_passes", "-n", type=int, default=1, help="Number of forward passes to run.")
    parser.add_argument("--vertical_level", type=int, default=72, help="Vertical level to evaluate on")
    args = parser.parse_args()

    # load trained AQcGAN model
    config_filepath = Path(args.config_filepath)
    trainer = parse_yaml(Path(""),config_filepath, args.chkpt_idx, device=DEVICE)
    trainer.gen.eval()

    # get meta data
    meta_data = read_pickle_file(args.meta_filepath)
    if not meta_data:
        sys.exit()

    # get dataset
    dataset = trainer.val_data

    old_feats = dataset.feat_names
    old_targets = dataset.target_names
    new_feats = {}
    new_targets = {}

    # Ensuring dataset has single vertical level (required for evaluation)
    # Remapping the correct input/target features to be for this vertical level 
    level = list(old_feats.keys())[0]
    new_feats = {args.vertical_level: []}
    for x in old_feats[level]:
        new_x = x.replace(str(level), str(args.vertical_level))
        # features that are vertical level dependent have the name "variable_level"
        # get the features for the new vertical level
        if new_x in meta_data['z_vars']:
            new_feats[args.vertical_level].append(new_x)
        else:
            new_feats[args.vertical_level].append(x)

    # repeat for target/output variables
    new_targets = {args.vertical_level: []}
    for x in old_targets[level]:
        new_x = x.replace(str(level), str(args.vertical_level))
        if new_x in meta_data['z_vars']:
            new_targets[args.vertical_level].append(new_x)
        else:
            new_targets[args.vertical_level].append(x)

    # use these to evaluate model on the desired vertical level
    # (potentially different than what it was trained on)
    dataset.dataset.setup_feats_targets(new_feats, new_targets)

    # Get normalization stats
    mu, sigma = get_norm_stats(dataset.target_names[args.vertical_level], meta_data)

    # returns mean and variance in normalized space across ensembles for each example in test set
    predictions = get_predictions(trainer, n_passes=args.n_passes)

    # invert mean and spread to get mean and std deviation in native space
    predictions_inv = inv_pred(predictions.numpy(), mu, sigma)

    # save predictions as nc4 files
    # filename has the form {exp_name}.aqcgan_prediction.{fcst init time}.nc4
    # fcst init time is the last timestamp of the GEOS species (only) data 
    # used for the forecast.
    # So if we use the data for 20230401 (0z-21z) as the initial condition, 
    # then the fcst init time is 20240402_21z
    exp_name = meta_data['exp_name']
    times =  meta_data['time_init']

    window_size = dataset.window_size
    time_span   = dataset.n_timesteps - dataset.window_size*(args.n_passes + 1) + 1
    # this is the last time of the GEOS-CF composition data used for a forecast
    fcst_init_times = times[window_size-1:
                            dataset.n_timesteps-window_size*args.n_passes]
    
    # Forecast lead times for the 8 n_frames
    fcst_lead_times = np.arange( 3 * window_size*(args.n_passes - 1) + 3,
                                 3 * window_size*args.n_passes + 3, 
                                 3, 
                                 dtype='timedelta64[h]' )


    # variable encoding
    encoding_dict = {"dtype": "float32", "zlib": True, "complevel": 3, '_FillValue':np.nan}
    
    # loop over fcst init times
    for ft in range(len(fcst_init_times)):
        ts = pd.Timestamp(fcst_init_times[ft]).strftime("%Y%m%d_%Hz")
        save_file = f"{exp_name}.aqcgan_prediction.{ts}.nc4"

        ds = xr.Dataset(
                data_vars={
                    **{var_name: (['time','lat','lon'], predictions_inv[ft,i,:,:,:]) 
                    for i, var_name in enumerate(dataset.target_names[args.vertical_level])
                    },
                    'fcst_lead_time': (['time'], fcst_lead_times )
                    },
                coords= {
                    'time': fcst_init_times[ft] + fcst_lead_times,
                    'lat': meta_data['lat'],
                    'lon': meta_data['lon'],
                    }
                )
    
        ds.attrs['title'] = 'AQcGAN predictions'
    
        if os.path.exists(trainer.chkpt_dir / save_file):
            print(f'Appending {save_file}')
            with xr.open_dataset( trainer.chkpt_dir / save_file, 
                                  decode_timedelta=True ) as ds_prev: 
                ds = ds_prev.combine_first(ds)
    
        ds.to_netcdf(trainer.chkpt_dir / save_file, engine='h5netcdf', mode="w", 
                        encoding={var: encoding_dict for var in ds.data_vars})